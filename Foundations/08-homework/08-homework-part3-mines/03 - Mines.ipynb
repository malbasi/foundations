{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping mine data with `.apply`\n",
    "\n",
    "## The pages we'll be looking at\n",
    "\n",
    "If I wanted to read specific information about a specfic mine, it takes a few steps. **Do these steps with your browser before you try any programming.**\n",
    "\n",
    "1. Visit the [Mine Data Retrieval System](https://arlweb.msha.gov/drs/drshome.htm)\n",
    "2. Scroll down to **Mine Identification Number (ID) Search**\n",
    "3. Type in a mine ID number, such as `3503598`, click **Search**\n",
    "4. I'm on a page! It lists the MINE NAME and MINE OWNER.\n",
    "\n",
    "After searching for and finding a mine, I can use this page to **find reports about this mine**. Some of the reports are on accidents, violations, inspections, health samples and more. To get those reports:\n",
    "\n",
    "1. Search for a mine (if you haven't already)\n",
    "2. Scroll down and change **Beginning Date** to `1/1/1995` (violation reports begin in 1995, accidents begin in 1983)\n",
    "3. Select the report type of `Violations`\n",
    "4. Click **Get Report**\n",
    "5. I'm on a page! It lists ALL OF THE MINE'S VIOLATIONS.\n",
    "\n",
    "By changing the report type you're searching for you can find all sorts of different data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing this programmatically\n",
    "\n",
    "## First, scraping a single page\n",
    "\n",
    "### Import your imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for a mine\n",
    "\n",
    "Visit the [Mine Data Retrieval System](https://arlweb.msha.gov/drs/drshome.htm) and use Selenium to search for `3503598`\n",
    "\n",
    "- *TIP: You might need to use the Selenium code to scroll down to the right spot on the page. Or not!*\n",
    "- *TIP: Use `.send_keys` to type into the box*\n",
    "- *TIP: On pages that never change, you can usually just use XPath if you're feeling lazy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://arlweb.msha.gov/drs/drshome.htm'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mine_id = 3503598\n",
    "id_box = driver.find_element_by_name('MineId')\n",
    "id_box.send_keys(mine_id)\n",
    "search_button = driver.find_element_by_xpath('//*[@id=\"content\"]/table[3]/tbody/tr[3]/td[2]/input')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding reports\n",
    "\n",
    "On the \"Report Selection Page\" (where you should be after you search), use Selenium to...\n",
    "\n",
    "- Change the **Beginning Date** to `1/1/1995`\n",
    "- Select the report type of `Violations`\n",
    "- Click **Get Report**\n",
    "\n",
    ".\n",
    "\n",
    "- *TIP: Remember, if someone isn't on the page Selenium can't click it!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_box = driver.find_element_by_name('BDate')\n",
    "date = \"1/1/1995\"\n",
    "date_box.send_keys(date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_box = driver.find_element_by_xpath('//*[@id=\"content\"]/form[1]/table[3]/tbody/tr[2]/td[2]/table/tbody/tr[1]/td/input')\n",
    "violations_box.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_report = driver.find_element_by_xpath('//*[@id=\"content\"]/form[1]/table[3]/tbody/tr[3]/td[2]/input')\n",
    "get_report.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving reports\n",
    "\n",
    "Save all of the rows of data on that page into a new dataframe. Each column is its own column, **and you also need to save the URL under the 'Standard' column.** Here, I even made you a blank dictionary:\n",
    "\n",
    "```python\n",
    "data = {}\n",
    "data['violator'] = ''\n",
    "data['contract_id'] = ''\n",
    "data['citation_no'] = ''\n",
    "data['case_no'] = ''\n",
    "data['date_issues'] = ''\n",
    "data['final_order_date'] = ''\n",
    "data['section_of_act'] = ''\n",
    "data['date_terminated'] = ''\n",
    "data['citation'] = ''\n",
    "data['s_and_s'] = ''\n",
    "data['standard'] = ''\n",
    "data['standard_url'] = ''\n",
    "data['proposed_penalty'] = ''\n",
    "data['citation_status'] = ''\n",
    "data['current_penalty'] = ''\n",
    "data['amount_paid'] = ''\n",
    "```\n",
    "\n",
    "- *TIP: Some of those table rows aren't what you want. How can you tell them apart from the good ones? (the previous mine owner ones are okay, I just mean the weird headers)*\n",
    "- *TIP: I sense `.find_elements` + a lot of square brackets*\n",
    "- *TIP: This is just like scraping a search results page!*\n",
    "- *TIP: For the URL, you'll need to find the `a` inside of the cell*\n",
    "- *TIP: class name is sadly not going to save your life here, because some of the `tr`s and `td`s have the same class! It's stupid. But there's a trick: CSS selectors! Something like `div#container` finds a `div` with the id of `container`, while `span.important` finds a `span` with the class of `important`. It should be helpful! And use `.find_elements_by_` + tab to see what the command is*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount_paid</th>\n",
       "      <th>case_no</th>\n",
       "      <th>citation</th>\n",
       "      <th>citation_no</th>\n",
       "      <th>citation_status</th>\n",
       "      <th>contract_id</th>\n",
       "      <th>current_penalty</th>\n",
       "      <th>date_issues</th>\n",
       "      <th>date_terminated</th>\n",
       "      <th>final_order_date</th>\n",
       "      <th>proposed_penalty</th>\n",
       "      <th>s_and_s</th>\n",
       "      <th>section_of_act</th>\n",
       "      <th>standard</th>\n",
       "      <th>standard_url</th>\n",
       "      <th>violator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116.00</td>\n",
       "      <td>000440809</td>\n",
       "      <td>C</td>\n",
       "      <td>8992368</td>\n",
       "      <td>Closed</td>\n",
       "      <td></td>\n",
       "      <td>116.00</td>\n",
       "      <td>5/2/2017</td>\n",
       "      <td>5/2/2017</td>\n",
       "      <td>7/23/2017</td>\n",
       "      <td>116.00</td>\n",
       "      <td>N</td>\n",
       "      <td>104(a)</td>\n",
       "      <td>56.14107(a)</td>\n",
       "      <td>http://www.gpo.gov/fdsys/pkg/CFR-2017-title30-...</td>\n",
       "      <td>Newberg Rock &amp; Dirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.00</td>\n",
       "      <td>000435717</td>\n",
       "      <td>C</td>\n",
       "      <td>8992214</td>\n",
       "      <td>Closed</td>\n",
       "      <td></td>\n",
       "      <td>116.00</td>\n",
       "      <td>2/7/2017</td>\n",
       "      <td>2/8/2017</td>\n",
       "      <td>5/21/2017</td>\n",
       "      <td>116.00</td>\n",
       "      <td>N</td>\n",
       "      <td>104(a)</td>\n",
       "      <td>56.14107(a)</td>\n",
       "      <td>http://www.gpo.gov/fdsys/pkg/CFR-2017-title30-...</td>\n",
       "      <td>Newberg Rock &amp; Dirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.00</td>\n",
       "      <td>000398910</td>\n",
       "      <td>C</td>\n",
       "      <td>8872462</td>\n",
       "      <td>Closed</td>\n",
       "      <td></td>\n",
       "      <td>100.00</td>\n",
       "      <td>10/21/2015</td>\n",
       "      <td>10/27/2015</td>\n",
       "      <td>1/13/2016</td>\n",
       "      <td>100.00</td>\n",
       "      <td>N</td>\n",
       "      <td>104(a)</td>\n",
       "      <td>56.14107(a)</td>\n",
       "      <td>http://www.gpo.gov/fdsys/pkg/CFR-2015-title30-...</td>\n",
       "      <td>Newberg Rock &amp; Dirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.00</td>\n",
       "      <td>000398910</td>\n",
       "      <td>C</td>\n",
       "      <td>8872461</td>\n",
       "      <td>Closed</td>\n",
       "      <td></td>\n",
       "      <td>100.00</td>\n",
       "      <td>10/21/2015</td>\n",
       "      <td>10/27/2015</td>\n",
       "      <td>1/13/2016</td>\n",
       "      <td>100.00</td>\n",
       "      <td>N</td>\n",
       "      <td>104(a)</td>\n",
       "      <td>56.12004</td>\n",
       "      <td>http://www.gpo.gov/fdsys/pkg/CFR-2015-title30-...</td>\n",
       "      <td>Newberg Rock &amp; Dirt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  amount_paid    case_no citation citation_no citation_status contract_id  \\\n",
       "0         NaN        NaN      NaN         NaN             NaN         NaN   \n",
       "1     116.00   000440809        C   8992368            Closed               \n",
       "2     116.00   000435717        C   8992214            Closed               \n",
       "3     100.00   000398910        C   8872462            Closed               \n",
       "4     100.00   000398910        C   8872461            Closed               \n",
       "\n",
       "  current_penalty date_issues date_terminated final_order_date  \\\n",
       "0             NaN         NaN             NaN              NaN   \n",
       "1         116.00     5/2/2017        5/2/2017       7/23/2017    \n",
       "2         116.00     2/7/2017        2/8/2017       5/21/2017    \n",
       "3         100.00   10/21/2015      10/27/2015       1/13/2016    \n",
       "4         100.00   10/21/2015      10/27/2015       1/13/2016    \n",
       "\n",
       "  proposed_penalty s_and_s section_of_act     standard  \\\n",
       "0              NaN     NaN            NaN          NaN   \n",
       "1           116.00       N         104(a)  56.14107(a)   \n",
       "2           116.00       N         104(a)  56.14107(a)   \n",
       "3           100.00       N         104(a)  56.14107(a)   \n",
       "4           100.00       N         104(a)     56.12004   \n",
       "\n",
       "                                        standard_url             violator  \n",
       "0                                                NaN                  NaN  \n",
       "1  http://www.gpo.gov/fdsys/pkg/CFR-2017-title30-...  Newberg Rock & Dirt  \n",
       "2  http://www.gpo.gov/fdsys/pkg/CFR-2017-title30-...  Newberg Rock & Dirt  \n",
       "3  http://www.gpo.gov/fdsys/pkg/CFR-2015-title30-...  Newberg Rock & Dirt  \n",
       "4  http://www.gpo.gov/fdsys/pkg/CFR-2015-title30-...  Newberg Rock & Dirt  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = driver.find_element_by_css_selector('#content > table.drsviols')\n",
    "rows = tables.find_elements_by_xpath('//*[@id=\"content\"]/table[5]/tbody/tr')\n",
    "violations_table = []\n",
    "\n",
    "for row in rows:\n",
    "    columns = row.find_elements_by_tag_name('td')\n",
    "    data = {}\n",
    "    try:\n",
    "        data['violator'] = columns[0].text\n",
    "        data['contract_id'] = columns[1].text\n",
    "        data['citation_no'] = columns[2].text\n",
    "        data['case_no'] = columns[3].text\n",
    "        data['date_issues'] = columns[4].text\n",
    "        data['final_order_date'] = columns[5].text\n",
    "        data['section_of_act'] = columns[6].text\n",
    "        data['date_terminated'] = columns[7].text\n",
    "        data['citation'] = columns[8].text\n",
    "        data['s_and_s'] = columns[9].text\n",
    "        data['standard'] = columns[10].text\n",
    "        data['standard_url'] = columns[10].find_element_by_tag_name('a').get_attribute('href')\n",
    "        data['proposed_penalty'] = columns[11].text\n",
    "        data['citation_status'] = columns[12].text\n",
    "        data['current_penalty'] = columns[13].text\n",
    "        data['amount_paid'] = columns[14].text\n",
    "    except:\n",
    "        pass\n",
    "    violations_table.append(data)\n",
    "# print(violations_table)\n",
    "df_violations = pd.DataFrame(violations_table)\n",
    "df_violations.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving that data\n",
    "\n",
    "Save the dataframe to a CSV file called `3503598-violations.csv` (that's the TDLR code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_violations.to_csv('3503598-violations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put that all in ONE cell that runs correctly\n",
    "\n",
    "The **entire process**, from searching to saving as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the webpage\n",
    "url = 'https://arlweb.msha.gov/drs/drshome.htm'\n",
    "driver.get(url)\n",
    "#define information, fill search boxes for mine ID, click search\n",
    "mine_id = 3503598\n",
    "id_box = driver.find_element_by_name('MineId')\n",
    "id_box.send_keys(mine_id)\n",
    "search_button = driver.find_element_by_xpath('//*[@id=\"content\"]/table[3]/tbody/tr[3]/td[2]/input')\n",
    "search_button.click()\n",
    "#Fill out reports request form and send\n",
    "date_box = driver.find_element_by_name('BDate')\n",
    "date = \"1/1/1995\"\n",
    "date_box.send_keys(date)\n",
    "\n",
    "violations_box = driver.find_element_by_xpath('//*[@id=\"content\"]/form[1]/table[3]/tbody/tr[2]/td[2]/table/tbody/tr[1]/td/input')\n",
    "violations_box.click()\n",
    "\n",
    "get_report = driver.find_element_by_xpath('//*[@id=\"content\"]/form[1]/table[3]/tbody/tr[3]/td[2]/input')\n",
    "get_report.click()\n",
    "\n",
    "#Find table on violations page\n",
    "tables = driver.find_element_by_css_selector('#content > table.drsviols')\n",
    "rows = tables.find_elements_by_xpath('//*[@id=\"content\"]/table[5]/tbody/tr')\n",
    "violations_table = []\n",
    "\n",
    "#cycle through rows and scrape information\n",
    "for row in rows:\n",
    "    columns = row.find_elements_by_tag_name('td')\n",
    "    data = {}\n",
    "    try:\n",
    "        data['violator'] = columns[0].text\n",
    "        data['contract_id'] = columns[1].text\n",
    "        data['citation_no'] = columns[2].text\n",
    "        data['case_no'] = columns[3].text\n",
    "        data['date_issues'] = columns[4].text\n",
    "        data['final_order_date'] = columns[5].text\n",
    "        data['section_of_act'] = columns[6].text\n",
    "        data['date_terminated'] = columns[7].text\n",
    "        data['citation'] = columns[8].text\n",
    "        data['s_and_s'] = columns[9].text\n",
    "        data['standard'] = columns[10].text\n",
    "        data['standard_url'] = columns[10].find_element_by_tag_name('a').get_attribute('href')\n",
    "        data['proposed_penalty'] = columns[11].text\n",
    "        data['citation_status'] = columns[12].text\n",
    "        data['current_penalty'] = columns[13].text\n",
    "        data['amount_paid'] = columns[14].text\n",
    "    except:\n",
    "        pass\n",
    "    violations_table.append(data)\n",
    "\n",
    "#save to dataframe\n",
    "df_violations = pd.DataFrame(violations_table)\n",
    "\n",
    "#save to csv\n",
    "df_violations.to_csv('3503598-violations.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using .apply to find data about SEVERAL mines\n",
    "\n",
    "The file `mines-subset.csv` has a list of mine IDs. We're going to scrape the operator's name for each of those mines.\n",
    "\n",
    "### Open up `mines-subset.csv` and save it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4104757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>801306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3609931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id\n",
       "0  4104757\n",
       "1   801306\n",
       "2  3609931"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset = pd.read_csv('mines-subset.csv')\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open up `mines-subset.csv` in a text editor, then look at your dataframe. Is something different about them? If so, make them match.\n",
    "\n",
    "- *TIP: You can zero fill if you want, but another option is that when reading in a CSV, `dtype='str'` will force everything to be a string*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4104757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0801306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3609931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id\n",
       "0  4104757\n",
       "1  0801306\n",
       "2  3609931"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset = pd.read_csv('mines-subset.csv', dtype='str')\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert your one-cell scraper into a function, and use it on each row of our dataset\n",
    "\n",
    "- *TIP: You'll be using `.apply`*\n",
    "- *TIP: You won't be joining this back into your dataframe, so you don't need to `return` anything or `join` or any of that.*\n",
    "- *TIP: Be careful of your **other variable names** - if you're calling the thing you're sending your function `row`, you can't use it anywhere else (like in your loop)*\n",
    "- *TIP: **BE CAREFUL WHAT YOU NAME YOUR DATAFRAMES.** If you name the citations dataframe `df` it can overwrite your mine ID `df`*\n",
    "- *TIP: You'll be saving a dataframe each time*\n",
    "- *TIP: Be sure you change everything that refers to the mine ID to refer to the current row's ID instead of `3503598`*\n",
    "- *TIP: BE SURE TO CHANGE EVERYTHING THAT REFERS TO THE MINE ID*\n",
    "- *TIP: EVERYTHING, EVERYTHING, EVERYTHING! Look at the end of your function! Maybe I'm overreacting, I don't know.*\n",
    "- *TIP: If you hit an error about list index out of range, see what line it's happening on and go look at the page. What's different about this page than the previous ones? (answer: the last three columns!) If you assign those columns later using `try`/`except` you should be able to get some data from those rows without throwing it all out. If you can't figure it out, just wrap it all in try/except and give up on those rows*\n",
    "- *TIP: Some of the standards might not have links, either, so you might want to wrap that in a `try`/`except`, too!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_scraper(data_row):\n",
    "    #Open the webpage\n",
    "    url = 'https://arlweb.msha.gov/drs/drshome.htm'\n",
    "    driver.get(url)\n",
    "    #define information, fill search boxes for mine ID, click search\n",
    "    mine_id = data_row\n",
    "    try:\n",
    "        id_box = driver.find_element_by_name('MineId')\n",
    "        id_box.send_keys(mine_id)\n",
    "        search_button = driver.find_element_by_xpath('//*[@id=\"content\"]/table[3]/tbody/tr[3]/td[2]/input')\n",
    "        search_button.click()\n",
    "        #Fill out reports request form and send\n",
    "        date_box = driver.find_element_by_name('BDate')\n",
    "        date = \"1/1/1995\"\n",
    "        date_box.send_keys(date)\n",
    "\n",
    "        violations_box = driver.find_element_by_xpath('//*[@id=\"content\"]/form[1]/table[3]/tbody/tr[2]/td[2]/table/tbody/tr[1]/td/input')\n",
    "        violations_box.click()\n",
    "\n",
    "        get_report = driver.find_element_by_xpath('//*[@id=\"content\"]/form[1]/table[3]/tbody/tr[3]/td[2]/input')\n",
    "        get_report.click()\n",
    "\n",
    "        #Find table on violations page\n",
    "        tables = driver.find_element_by_css_selector('#content > table.drsviols')\n",
    "        rows = tables.find_elements_by_xpath('//*[@id=\"content\"]/table[5]/tbody/tr')\n",
    "        violations_table = []\n",
    "\n",
    "        #cycle through rows and scrape information\n",
    "        for row in rows:\n",
    "            columns = row.find_elements_by_tag_name('td')\n",
    "            data = {}\n",
    "            try:\n",
    "                data['violator'] = columns[0].text\n",
    "                data['contract_id'] = columns[1].text\n",
    "                data['citation_no'] = columns[2].text\n",
    "                data['case_no'] = columns[3].text\n",
    "                data['date_issues'] = columns[4].text\n",
    "                data['final_order_date'] = columns[5].text\n",
    "                data['section_of_act'] = columns[6].text\n",
    "                data['date_terminated'] = columns[7].text\n",
    "                data['citation'] = columns[8].text\n",
    "                data['s_and_s'] = columns[9].text\n",
    "                data['standard'] = columns[10].text\n",
    "                data['standard_url'] = columns[10].find_element_by_tag_name('a').get_attribute('href')\n",
    "                data['proposed_penalty'] = columns[11].text\n",
    "                data['citation_status'] = columns[12].text\n",
    "                data['current_penalty'] = columns[13].text\n",
    "                data['amount_paid'] = columns[14].text\n",
    "            except:\n",
    "                pass\n",
    "            violations_table.append(data)\n",
    "\n",
    "        #save to dataframe\n",
    "        df_violations = pd.DataFrame(violations_table)\n",
    "\n",
    "        #save to csv\n",
    "        mine_id = str(mine_id)\n",
    "        csv_name = mine_id + \"-violations.csv\"\n",
    "        df_violations.to_csv(csv_name, index=False)\n",
    "        print('saved', csv_name)\n",
    "    except:\n",
    "        print('Mine Id not found')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "Name: id, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.id.apply(mine_scraper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okay, now do it for ALL of the mines\n",
    "\n",
    "Open up `mines.csv` using pandas and do the same thing, it will just be for more mines this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mines = pd.read_csv('mines.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "5     None\n",
       "6     None\n",
       "7     None\n",
       "8     None\n",
       "9     None\n",
       "10    None\n",
       "11    None\n",
       "12    None\n",
       "13    None\n",
       "14    None\n",
       "15    None\n",
       "16    None\n",
       "17    None\n",
       "18    None\n",
       "Name: id, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mines.id.apply(mine_scraper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
